\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{npg-27-373-2020}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}The Lorenz System}{4}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Lorenz attractor for $\sigma = 10$, $\rho = 28$, and $\beta = \frac  {8}{3}$. Generated with the RK4 method for 100 time units}}{4}{figure.1}\protected@file@percent }
\newlabel{fig:lorenz_attractor}{{1}{4}{The Lorenz attractor for $\sigma = 10$, $\rho = 28$, and $\beta = \frac {8}{3}$. Generated with the RK4 method for 100 time units}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Neural Networks}{5}{subsection.2.2}\protected@file@percent }
\citation{Goodfellow-et-al-2016}
\citation{Goodfellow-et-al-2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}RNNs}{6}{subsection.2.3}\protected@file@percent }
\citation{StanfordEngineering2020}
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An architecture of a recurrent neural network (adapted from \cite  {Goodfellow-et-al-2016}).}}{7}{figure.2}\protected@file@percent }
\newlabel{fig:rnn_diagram}{{2}{7}{An architecture of a recurrent neural network (adapted from \cite {Goodfellow-et-al-2016})}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Diagram of LSTM cell (adapted from \cite  {StanfordEngineering2020})}}{8}{figure.3}\protected@file@percent }
\newlabel{fig:lstm_diagram}{{3}{8}{Diagram of LSTM cell (adapted from \cite {StanfordEngineering2020})}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Transformers}{8}{subsection.2.4}\protected@file@percent }
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Multi-Head Attention mechaism (adapted from \cite  {DBLP:journals/corr/VaswaniSPUJGKP17})}}{9}{figure.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Reservoir Computing and Echo State Networks}{9}{subsection.2.5}\protected@file@percent }
\citation{reservoirpy}
\citation{github}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Echo State Network architecture (adapted from \cite  {reservoirpy}).}}{10}{figure.5}\protected@file@percent }
\newlabel{fig:esn}{{5}{10}{Echo State Network architecture (adapted from \cite {reservoirpy})}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Method}{11}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generating Data}{11}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Setting up the LSTM RNN}{12}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Setting up the Transformers Model}{13}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Setting up the RC-ESN}{13}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Hyperparameter optimization process}{13}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{15}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Lorenz attractor}{15}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LSTM RNN}}{15}{figure.6}\protected@file@percent }
\newlabel{fig:rnn_lorenz}{{6}{15}{LSTM RNN}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces ESN}}{15}{figure.7}\protected@file@percent }
\newlabel{fig:echo_lorenz}{{7}{15}{ESN}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Transformers}}{15}{figure.8}\protected@file@percent }
\newlabel{fig:transformers_lorenz}{{8}{15}{Transformers}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Trajectory}{15}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}MSE}{15}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{15}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces LSTM RNN path (true path is red, model path is blue)}}{16}{figure.9}\protected@file@percent }
\newlabel{fig:rnn_path}{{9}{16}{LSTM RNN path (true path is red, model path is blue)}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces ESN path(true path is red, model path is blue)}}{16}{figure.10}\protected@file@percent }
\newlabel{fig:echo_path}{{10}{16}{ESN path(true path is red, model path is blue)}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Transformers path(true path is red, model path is blue)}}{16}{figure.11}\protected@file@percent }
\newlabel{fig:transformers_path}{{11}{16}{Transformers path(true path is red, model path is blue)}{figure.11}{}}
\citation{npg-27-373-2020}
\citation{cite-key}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Conclusion}{17}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Code snippets}{18}{appendix.A}\protected@file@percent }
\newlabel{appendix:code}{{A}{18}{Code snippets}{appendix.A}{}}
\newlabel{rk4:lorenz}{{1}{18}{RK4 implementation}{lstlisting.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {1}RK4 implementation}{18}{lstlisting.1}\protected@file@percent }
\newlabel{datasplit}{{2}{18}{get\_training\_data.py}{lstlisting.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {2}get\_training\_data.py}{18}{lstlisting.2}\protected@file@percent }
\newlabel{lstm}{{3}{19}{LSTM RNN implementation}{lstlisting.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3}LSTM RNN implementation}{19}{lstlisting.3}\protected@file@percent }
\newlabel{inverse:normalization}{{4}{20}{inverse normalization}{lstlisting.4}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {4}inverse normalization}{20}{lstlisting.4}\protected@file@percent }
\newlabel{training:loop}{{5}{20}{Training loop}{lstlisting.5}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5}Training loop}{20}{lstlisting.5}\protected@file@percent }
\newlabel{transformer}{{6}{21}{Transformer implementation}{lstlisting.6}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {6}Transformer implementation}{21}{lstlisting.6}\protected@file@percent }
\newlabel{positional:encoding}{{7}{22}{Positional encoding implementation}{lstlisting.7}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {7}Positional encoding implementation}{22}{lstlisting.7}\protected@file@percent }
\newlabel{reservoir}{{8}{22}{ESN implementation}{lstlisting.8}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {8}ESN implementation}{22}{lstlisting.8}\protected@file@percent }
\newlabel{true:loss}{{9}{23}{true\_loss.py}{lstlisting.9}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {9}true\_loss.py}{23}{lstlisting.9}\protected@file@percent }
\citation{DBLP:journals/corr/VaswaniSPUJGKP17}
\@writefile{toc}{\contentsline {section}{\numberline {B}Figures}{24}{appendix.B}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Transformers decoder architecture, (adapted from \cite  {DBLP:journals/corr/VaswaniSPUJGKP17})}}{24}{figure.12}\protected@file@percent }
\newlabel{fig:transformer_diagram}{{12}{24}{Transformers decoder architecture, (adapted from \cite {DBLP:journals/corr/VaswaniSPUJGKP17})}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces RNN MSE}}{25}{figure.13}\protected@file@percent }
\newlabel{fig:rnn_mse}{{13}{25}{RNN MSE}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces ESN MSE}}{25}{figure.14}\protected@file@percent }
\newlabel{fig:echo_mse}{{14}{25}{ESN MSE}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Transformers MSE}}{25}{figure.15}\protected@file@percent }
\newlabel{fig:transformers:mse}{{15}{25}{Transformers MSE}{figure.15}{}}
\bibstyle{apalike}
\bibdata{references}
\bibcite{npg-27-373-2020}{Chattopadhyay et\nobreakspace  {}al., 2020}
\bibcite{github}{Delgado, 2024}
\bibcite{cite-key}{Gauthier et\nobreakspace  {}al., 2021}
\bibcite{Goodfellow-et-al-2016}{Goodfellow et\nobreakspace  {}al., 2016}
\bibcite{reservoirpy}{Hinaut and Trouvain, 2021}
\bibcite{StanfordEngineering2020}{Justin\nobreakspace  {}Johnsson and Yeung, 2017}
\bibcite{DBLP:journals/corr/VaswaniSPUJGKP17}{Vaswani et\nobreakspace  {}al., 2017}
\gdef \@abspage@last{26}
