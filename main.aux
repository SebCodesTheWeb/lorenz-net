\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces https://qparticle.wordpress.com/2014/07/11/lorenz-attractor/}}{1}{figure.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Theory}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Lorenz System}{3}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Lorenz attractor for $\sigma = 10$, $\rho = 28$, and $\beta = \frac  {8}{3}$.}}{3}{figure.2}\protected@file@percent }
\newlabel{fig:lorenz_attractor}{{2}{3}{The Lorenz attractor for $\sigma = 10$, $\rho = 28$, and $\beta = \frac {8}{3}$}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Recurrent Neural Networks and LSTM}{3}{subsection.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces RNN architecture - https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/}}{4}{figure.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Transformers}{4}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Taken from "All you need is attention" by Google. }}{5}{figure.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Basic Transformers architecture, taken from "All you need is attention" by Google. }}{5}{figure.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Reservoir Computing and Echo State Networks}{6}{subsection.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces rservoirpy user guide docs}}{6}{figure.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generating Data}{6}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Autocorrelation shows how much the data is correlated with itself at different time lags}}{9}{figure.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Setting up the RNN}{9}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Setting up the transformers architecture}{12}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Setting up the RC-ESN}{14}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Training process}{14}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{15}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}RNN}{15}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces RNN Lorenz}}{16}{figure.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces RNN MSE}}{16}{figure.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces RNN path}}{17}{figure.10}\protected@file@percent }
\gdef \@abspage@last{17}
