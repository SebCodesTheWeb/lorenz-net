\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Theory}{1}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Lorenz System}{1}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The Lorenz attractor for $\sigma = 10$, $\rho = 28$, and $\beta = \frac  {8}{3}$.}}{2}{}\protected@file@percent }
\newlabel{fig:lorenz_attractor}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Recurrent Neural Networks and LSTM}{2}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RNN architecture - https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/}}{2}{}\protected@file@percent }
\newlabel{fig:lorenz_attractor}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Transformers}{3}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Taken from "All you need is attention" by Google. }}{3}{}\protected@file@percent }
\newlabel{fig:lorenz_attractor}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Basic Transformers architecture, taken from "All you need is attention" by Google. }}{4}{}\protected@file@percent }
\newlabel{fig:lorenz_attractor}{{4}{4}}
\gdef \@abspage@last{4}
